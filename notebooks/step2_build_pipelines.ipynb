{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```                \n",
    "  __  __ _       __ _                 _______             _    _                     ___  \n",
    " |  \\/  | |     / _| |               |__   __|           | |  (_)                   |__ \\ \n",
    " | \\  / | |    | |_| | _____      __    | |_ __ __ _  ___| | ___ _ __   __ _  __   __  ) |\n",
    " | |\\/| | |    |  _| |/ _ \\ \\ /\\ / /    | | '__/ _` |/ __| |/ / | '_ \\ / _` | \\ \\ / / / / \n",
    " | |  | | |____| | | | (_) \\ V  V /     | | | | (_| | (__|   <| | | | | (_| |  \\ V / / /_ \n",
    " |_|  |_|______|_| |_|\\___/ \\_/\\_/      |_|_|  \\__,_|\\___|_|\\_\\_|_| |_|\\__, |   \\_(_)____|\n",
    "                                                                        __/ |             \n",
    "                                                                       |___/              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### MLflow is an open source platform for managing the end-to-end machine learning lifecycle. \n",
    "\n",
    "#### It tackles three primary functions:\n",
    "\n",
    "    1) Tracking experiments to record and compare parameters and results (MLflow Tracking).\n",
    "    2) Packaging ML code in a reusable, reproducible form in order to share with other data scientists or transfer to production (MLflow Projects).\n",
    "    3) Managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms (MLflow Models).\n",
    "\n",
    "(source https://www.mlflow.org/docs/latest/index.html#mlflow-documentation)\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### This is the second version of the tutorial. We packed code into modules, some modules are pipeline modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mlflow-1-tracking\n"
     ]
    }
   ],
   "source": [
    "%cd /home/mlflow-1-tracking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': {'project': '7labs/mlflow-1-tracking',\n",
       "  'name': 'iris',\n",
       "  'tags': ['solution-0-prototype', 'dev'],\n",
       "  'model': {'model_name': 'model.joblib', 'models_folder': 'models'},\n",
       "  'experiments': {'experiments_folder': 'experiments'},\n",
       "  'random_state': 42},\n",
       " 'split_train_test': {'folder': 'experiments',\n",
       "  'train_csv': 'data/processed/train_iris.csv',\n",
       "  'test_csv': 'data/processed/test_iris.csv',\n",
       "  'test_size': 0.2},\n",
       " 'featurize': {'dataset_csv': 'data/raw/iris.csv',\n",
       "  'featured_dataset_csv': 'data/interim/featured_iris.csv',\n",
       "  'features_columns_range': ['sepal_length', 'petal_length_to_petal_width'],\n",
       "  'target_column': 'species'},\n",
       " 'train': {'cv': 5,\n",
       "  'estimator_name': 'LogisticRegression',\n",
       "  'estimators': {'LogisticRegression': {'param_grid': {'C': [0.001, 0.01],\n",
       "     'max_iter': [100],\n",
       "     'solver': ['lbfgs'],\n",
       "     'multi_class': ['multinomial']}},\n",
       "   'SVC': {'param_grid': {'C': [0.1, 1.0],\n",
       "     'kernel': ['rbf', 'linear'],\n",
       "     'gamma': ['scale'],\n",
       "     'degree': [3, 5]}}}},\n",
       " 'evaluate': {'metrics_file': 'eval.txt'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Look on pipelines config \n",
    "config = yaml.load(open('config/pipeline_config.yml'), Loader=yaml.FullLoader)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse folder with configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/pipelines/featurize.py \\\n",
    "    --config=config/pipeline_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featured_iris.csv\n"
     ]
    }
   ],
   "source": [
    "# iris dataset with new features is created\n",
    "!ls data/interim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/pipelines/split_train_test.py \\\n",
    "    --config=config/pipeline_config.yml\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_iris.csv  train_iris.csv\n"
     ]
    }
   ],
   "source": [
    "# train and test datsets are created\n",
    "!ls data/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train config\n",
    "!cat experiments/train_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "0.87024409313883\n"
     ]
    }
   ],
   "source": [
    "!python src/pipelines/train.py \\\n",
    "    --config=config/pipeline_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.joblib\n"
     ]
    }
   ],
   "source": [
    "# model is created\n",
    "!ls models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate config\n",
    "!cat experiments/evaluate_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_score': 0.9305555555555555, 'confusion_matrix': [[10, 0, 0], [0, 7, 0], [0, 2, 11]]}\n",
      "[<Experiment: artifact_location='file:///home/mlflow-1-tracking/mlruns/0', experiment_id='0', lifecycle_stage='active', name='LogisticRegression'>, <Experiment: artifact_location='file:///home/mlflow-1-tracking/mlruns/1', experiment_id='1', lifecycle_stage='active', name='SVC'>]\n",
      "<ActiveRun: >\n",
      "<RunInfo: artifact_uri='file:///home/mlflow-1-tracking/mlruns/0/29207897c5b041f6a946ba9226ba9979/artifacts', end_time=None, experiment_id='0', lifecycle_stage='active', run_id='29207897c5b041f6a946ba9226ba9979', run_uuid='29207897c5b041f6a946ba9226ba9979', start_time=1561384732293, status='RUNNING', user_id='user'>\n",
      "29207897c5b041f6a946ba9226ba9979\n"
     ]
    }
   ],
   "source": [
    "!python src/pipelines/evaluate.py \\\n",
    "    --config=config/pipeline_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval.txt\n"
     ]
    }
   ],
   "source": [
    "# metrics file eval.txt is created\n",
    "!ls experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"f1_score\": 0.9305555555555555,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      10,\n",
      "      0,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      0,\n",
      "      7,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      0,\n",
      "      2,\n",
      "      11\n",
      "    ]\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat experiments/eval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.9305555555555555,\n",
       " 'confusion_matrix': [[10, 0, 0], [0, 7, 0], [0, 2, 11]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_report = yaml.load(open('experiments/eval.txt'), Loader=yaml.FullLoader)\n",
    "evaluate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter MLflow ui\n",
    "## http://0.0.0.0:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluion\n",
    "\n",
    "    Here we used another way to logging\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### ___Your task___\n",
    "\n",
    "#### Train with another estimator\n",
    "\n",
    "##### 1. Open config/pipeline_config.yml\n",
    "##### 2. In section _train_ change _estimator_name_ to SVC\n",
    "##### 3. Rerun stages __Train__ and  __Evaluate__ \n",
    "##### 4. Go to next section __Enter MLflow ui__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
